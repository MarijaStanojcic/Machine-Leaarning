---
title: "Classification using Decision Trees and Rules"
author: "Marija Stanojcic"
date: "April 28, 2018"
output: html_document
---

# 1) Performing the Tree based analysis of the credit data. Indentify who should be approved for the credit.

The original data is from UCI Machine Learning Repository and it can be found here https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data).


The data used for this is a slightly modified data. 

# Step 1 - getting the data

```{r}
URL <- "http://www.sci.csueastbay.edu/~esuess/classes/Statistics_6620/Presentations/ml7/credit.csv"
download.file(URL, destfile = "credit.csv", method="curl")
```

\n
# Step 2 - exploring and preparing the data

```{r}
credit <- read.csv("credit.csv", stringsAsFactors = TRUE)
str(credit)
```

A closer look into the data.

```{r}
library(dplyr)
summary(select_if(credit, is.numeric))
```

```{r}
summary(select_if(credit, is.factor))
```

Percentage of the not approved and approved credit.
```{r}
round(prop.table(table(credit$default)) * 100, 2)
```


Percentage of not approved and approved credit for education purpose.
```{r}
education <- subset(credit, credit$purpose == "education")
round(prop.table(table(education$default)) * 100, 2)
```
38.98% of education loan were allowed.\n

## Splitting the data into training and test datasets.
sample.split() function splits data into two parts, with defined Split.Ratio. This function also preserves relative ratios of different labels in the target variable.
```{r}
library(caTools)
set.seed(123)
split = sample.split(credit$default, SplitRatio = 0.8)
training_credit = subset(credit, split == TRUE)
test_credit = subset(credit, split == FALSE)
```

Checking label ratios for the training and test set.
```{r}
print("Training set:")
round(prop.table(table(training_credit$default)) * 100, 2)
print("Test set:")
round(prop.table(table(test_credit$default)) * 100, 2)
```
They are the same as for the original data, because sample.split() function preserves the ratios.

# Step 3 - training the model

```{r}
# install.packages("C50")
library(C50)
credit_DT <- C5.0(training_credit[-17], training_credit$default)
credit_DT
```

```{r}
summary(credit_DT)
```

# Step 4 - evaluating model performance

```{r}
(credit_pred <- predict(credit_DT, test_credit))
```

Making the confusion matrix.
```{r}
library(caret)
(cm_credit = confusionMatrix(test_credit$default, credit_pred))
```

```{r}
round(cm_credit$overall['Accuracy'] * 100, 2)
```

Accuracy is 70%.
This isn't a good accuracy. We should try to improve the model.

# Step 5 - improving the model performance 


Boosting the model - process where many decision trees are built and the trees vote on the best class for each example.
```{r}
credit_boost10 <- C5.0(training_credit[-17], training_credit$default,
                       trials = 10) # trials = how many trees we want to build
credit_boost10
# summary(credit_boost10)
```

Predicting approval of the credit with boosted model. 
```{r}
(credit_boost10_pred <- predict(credit_boost10, test_credit))
```

Making a confusion matrix.
```{r}
(cm_boost10 = confusionMatrix(credit_boost10_pred, test_credit$default))
```

```{r}
round(cm_boost10$overall['Accuracy'] * 100, 2)
```

Accuracy after boosting is 75%. If we compare with the first model, accuracy did improve for 5%.

\n

# Cost of the mistakes

Making some mistakes can be more costly than others. 
For example, giving a loan to a candidate who is likely to __________ can be expensive.

Making a cost matrix such that if the model don't approve credit when it actually should be approved the cost is 4 times bigger.
```{r}
matrix_dimensions <- list(c("no", "yes"), c("no", "yes"))
names(matrix_dimensions) <- c("predicted", "actual")
(cost <- matrix(c(0, 1, 4, 0), nrow = 2, dimnames = matrix_dimensions))
```

\n
Applying the cost matrix to the tree.
```{r}
credit_cost <- C5.0(training_credit[-17], training_credit$default,
                    costs = cost)
summary(credit_cost)
```

```{r}
credit_cost_pred <- predict(credit_cost, test_credit)
(cm_cost = confusionMatrix(credit_cost_pred, test_credit$default))
```

Accuracy of this model is less the before, but now costs are different. The model did actually improve wrong predictions for not approving credit. Here we only have 8 of those wrong predictions.
\n
Boosting the cost model.
```{r}
credit_cost_boost10 <- C5.0(training_credit[-17], training_credit$default,
                    costs = cost, trials = 10)
# summary(credit_cost_boost10)
```


```{r}
credit_cost_pred_boost10 <- predict(credit_cost_boost10, test_credit)
(cm_cost_boost10 = confusionMatrix(credit_cost_pred_boost10, test_credit$default))
```

This model has the highest accuracy of 76.5%, which still isn't great. 
### Cost are bad :(

# 2) Perform the Rule based analysis of the mushroom data. Identifying Poisonous Mushrooms. 

The data is Mushroom dataset from UCI Machine Learning Repository. The data can be found here https://archive.ics.uci.edu/ml/datasets/Mushroom.

Description of the data from the website.
\n
This data set includes descriptions of hypothetical samples corresponding to 23 species of gilled mushrooms in the Agaricus and Lepiota Family (pp. 500-525). Each species is identified as definitely edible, definitely poisonous, or of unknown edibility and not recommended. This latter class was combined with the poisonous one. The Guide clearly states that there is no simple rule for determining the edibility of a mushroom; no rule like "leaflets three, let it be" for Poisonous Oak and Ivy.
\n


# Step 1 - getting the data
```{r}
URL <- "http://www.sci.csueastbay.edu/~esuess/classes/Statistics_6620/Presentations/ml8/mushrooms.csv"
download.file(URL, destfile = "./mushrooms.csv", method="curl")
```

# Step 2 - exploring and preparing the data 

```{r}
mushrooms <- read.csv("mushrooms.csv", stringsAsFactors = TRUE)
str(mushrooms)
```
Variable veil_type has only one category, so it should be dropped as it isn't significant for the model.


```{r}
mushrooms$veil_type <- NULL
head(mushrooms)
```

A closer look into the data.
```{r}
summary(mushrooms)
```

Proportion of edible and poisonous mushrooms in the data.
```{r}
round(prop.table(table(mushrooms$type)) * 100, 2)
```
\n
## Splitting the data into training and test datasets.

```{r}
# library(caTools)
set.seed(123)
split2 = sample.split(mushrooms$type, SplitRatio = 0.75)
training_mushrooms = subset(mushrooms, split == TRUE)
test_mushrooms = subset(mushrooms, split == FALSE)

print("Training set:")
round(prop.table(table(training_mushrooms$type)) * 100, 2)
print("Test set:")
round(prop.table(table(test_mushrooms$type)) * 100, 2)
```

# Step 3 - training the model

```{r}
# install.packages("RWeka")
library(RWeka)
```

Train OneR() on the data.

```{r}
mushroom_1R <- OneR(type ~ ., data = training_mushrooms)
mushroom_1R
```

```{r}
summary(mushroom_1R)
```
Even though 1R classifier on the training set didn't classify any edible mushrooms as poisonous, it did classify 91 poisonous mushrooms as edible. This is a really dangerous mistake!
\n
```{r}
mushroom_1R_pred <- predict(mushroom_1R, test_mushrooms)
head(mushroom_1R_pred)
```

```{r}
(cm_mushrooms <- confusionMatrix(mushroom_1R_pred, test_mushrooms$type))
```
Accuracy is really good - 98.21%, but the classifier did classify 29 poisonous mushrooms as edible. 

# Step 5 - improving the model performance

Using JRip(), Java based implementation of the RIPPER rule. 
```{r}
(mushroom_JRip <- JRip(type ~ ., data = mushrooms))
```
JRip classifier learned 9 rules.

```{r}
summary(mushroom_JRip)
```

```{r}
mushroom_JRip_pred <- predict(mushroom_JRip, test_mushrooms)
head(mushroom_JRip_pred)
```

```{r}
confusionMatrix(mushroom_JRip_pred, test_mushrooms$type)
```

The classifier achieved 100% accuracy, this is because each mushroom was unique enough and the classifier was able to correctly classify it.















